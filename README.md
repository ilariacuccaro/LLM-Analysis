# LLM-Analysis
## 1. Project Description
The prototype aims to support the analysis of texts through the use of machine learning models, in particular LLM models (Large Language Models).
The purpose of the prototype is to analyze a CSV file containing a series of texts (e.g. abstracts of scientific papers) from a set of default queries automatically executed by the LLM model. Questions may concern keyword extraction requests, summarization, degree of consistency with a specific topic, etc. The results of the queries will be saved to a NoSQL database.
## 2. Prototype functionalities:
- Read data from existing JSON (or CSV) files
- Read a list of questions
- Interatively submit the question on each element of the input
- Save the question results into the database
## 3. Prototype design
![image](https://github.com/Doan314/LLM-Analysis/assets/160939362/d1b18d4a-e56f-4dff-bd4a-c246a4deb08c)
- Data and question files are loaded to **MongoDB** - a NoSQL database
- The data file is read by the LLM protytype, which uses
  - **Anaconda**: An open-source environment that includes a wide range of tools andlibraries. It is designed to simplify the installation and management of these tools, allowing users to easily create isolated environmentsto develop and run Python projects without having to manually manage software dependencies, thanks to a package manager called “conda”.
  - **PyTorch**: An open-source machine learning framework that provides capabilities for the creation and training of neural networks.
  - **LLAMA-CPP-Python**: A library that provides easy access and use of large languagetemplates such as .gguf files, which runs efficiently in CPU-only or mixed CPU/GPU environments.
  - **CUDA**: a parallel computing platform developed by NVIDIA, which enables the use of GPUs for computing operations.- Onprem: A Python library to run Large Language Model locally.
  - **Jupyter Notebook**: is an open-source platform that allows users to create and share interactive documents called "notebooks" using a web browser, this means that you can access and work notebooks from any computer connected to the Internet. Jupyternotebooks allow you to combine descriptive text, executable code (in languages like Python, R, Julia) and results (like charts or tables) within a single user interface.
- The answer generated by the LLM will be saved into MongoDB in JSON format, using **Simple File Connector**.
---
### 3.1. Access MongoDB database
To connect to the database, it is best to download the [MongoDb Compass tool](https://www.mongodb.com/products/tools/compass) from the manufacturer's official website. Once installed, enter the given URL:
> mongodb+srv://bigdata:ProjectBigData123@llm.gsegixp.mongodb.net/

Then save the database name under any name of your choice and press connect.
> [!NOTE]  
> The database does not work with UnicamEasyWifi. Use a different internet connection.
---
### 3.2. Implementation Large Language Model
*The following instructions refer to an installation with Windows*

**STEP 1: installed the requirements packages for Onprem within Anaconda**
- Donwload [Anaconda](https://docs.anaconda.com/free/anaconda/install/windows/)
- Create LLM environment in Anaconda
- Use [CUDA](https://developer.nvidia.com/cuda-downloads) to take advantage of the performance of NVIDIA GPUs (cuBlas library)
- Install [PyTorch](https://pytorch.org/get-started/locally/) for capabilities for the creation and training of neural networks
  ```
  conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
  ```
- Install [llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp#installation)
  ```
  set CMAKE_ARGS=-DLLAMA_CUBLAS=on
  set FORCE_CMAKE=1  
  pip install llama-cpp-python
  ```

**STEP 2: Install [Onprem](https://github.com/amaiya/onprem)**
  ```
  pip install onprem
  ```

**STEP 3: Check if Jupyter is installed in Anaconda**
- Select LLM kernel

**STEP 4 : Speeding Up Inference Using a [GPU](https://python.langchain.com/docs/integrations/llms/llamacpp)**
Modified two of parameters from the [core.py](https://github.com/Doan314/LLM-Analysis/blob/main/Library/Onprem/core.py) file:
- n_gpu_layers → how many layers of the model are downloaded to the GPU, optimal choice is `int = 24`
- n_batch → how many tokens are processed in parallel, optimal choice is `int = 800`

**STEP 5: Adapt the model to the proposed problem**
  - Check if the **langchain package** is installed
  - Since the model has to answer one item at a time, a loop is created to iterates the sequence of the documents, for each of the call `self.combine_documents_chain.run()` is executed and adds the result to the answer list.

From langchain\chains\retrieval_qa\base.py, _call() function:
  ```
    answer = []
        for document in docs:
            answer.append(self.combine_documents_chain.run(
                input_documents=document, question=question, callbacks=_run_manager.get_child()
            )
            )
  ```

**STEP 6: "Talk to Your Documents" section of Onprem**
- Create New Notebook (Run.ipynb file) inside Jupyter

  So as to be able to insert executable code and to visualize the output generated from the execution of such code.
- For responses to be generated from the content of our document, it is important to create an instance of the LLM class, defined in the Onprem [core.py](https://github.com/Doan314/LLM-Analysis/blob/main/Library/Onprem/core.py) file
  ```
  llm = LLM() 
  ```
- Pass the file CSV containing the articles and ingests document in source_directory. This line calls the ingest method to process and "ingest" the CSV file data containedin the specified directory, namely our articles
  ```
  llm.ingest(file_path_dataset)  
  ```
- Open the specified JSON file containing the questions for reading and load the contents into a Python dictionary with open(file_path_questions, 'r') as file:
  ```
  content= json.load(file)
  all_question= content["questions"]
  ```
- For each question in the question list, the ask() method is called, which will process the question using the natural language functionality provided by LLM and returns the result, which is then stored in the “result” dictionary.
  ```
  for question in all_question:
  result_question= llm.ask(question)
  ```
> [!NOTE]  
> The LLM template processes requests in parallel and therefore the order in which the answers are provided could correspond to the original order of the items.
>
> Step 5 and 6 can be inverted.

**STEP 7: Save result in a json file**

**STEP 8: Send the saved result in a JSON file to the MongoDB database**
```
  mongodb_uri = "mongodb+srv://bigdata:ProjectBigData123@llm.gsegixp.mongodb.net/"  
  database_name = "BigData"
  collection_name = "Output"
```
